{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (sub-sentence level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference pipeline - Proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load imports and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import MultiTagger\n",
    "from flair.models import TextClassifier\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-11 13:21:24,330 --------------------------------------------------------------------------------\n",
      "2021-03-11 13:21:24,333 The model key 'chunk-fast' now maps to 'https://huggingface.co/flair/chunk-english-fast' on the HuggingFace ModelHub\n",
      "2021-03-11 13:21:24,334  - The most current version of the model is automatically downloaded from there.\n",
      "2021-03-11 13:21:24,337  - (you can alternatively manually download the original model at https://nlp.informatik.hu-berlin.de/resources/models/chunk-fast/en-chunk-conll2000-fast-v0.4.pt)\n",
      "2021-03-11 13:21:24,338 --------------------------------------------------------------------------------\n",
      "2021-03-11 13:21:24,684 loading file /Users/ike/.flair/models/chunk-english-fast/be3a207f4993dd6d174d5083341a717d371ec16f721358e7a4d72158ebab28a6.a7f897d05c83e618a8235bbb7ddfca5a79d2daefb8a97c776eb73f97dbaea508\n",
      "2021-03-11 13:21:26,587 --------------------------------------------------------------------------------\n",
      "2021-03-11 13:21:26,588 The model key 'pos-fast' now maps to 'https://huggingface.co/flair/pos-english-fast' on the HuggingFace ModelHub\n",
      "2021-03-11 13:21:26,592  - The most current version of the model is automatically downloaded from there.\n",
      "2021-03-11 13:21:26,594  - (you can alternatively manually download the original model at https://nlp.informatik.hu-berlin.de/resources/models/pos-fast/en-pos-ontonotes-fast-v0.5.pt)\n",
      "2021-03-11 13:21:26,597 --------------------------------------------------------------------------------\n",
      "2021-03-11 13:21:26,849 loading file /Users/ike/.flair/models/pos-english-fast/36f7923039eed4c66e4275927daaff6cd275997d61d238355fb1fe0338fe10a1.ff87e5b4e47fdb42a0c00237d9506c671db773e0a7932179ace82e584383a1b8\n"
     ]
    }
   ],
   "source": [
    "# load the 'chunk' and POS taggers\n",
    "tagger = MultiTagger.load(['chunk-fast', 'pos-fast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-11 13:21:28,742 loading file /Users/ike/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "# load sentiment tagger\n",
    "classifier = TextClassifier.load('sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a sentence\n",
    "text = 'I love Bamboo HR, but interviews make me nervous'\n",
    "\n",
    "# text = 'I think sentiment analysis is really cool but maybe too cool'\n",
    "# text = 'I like the Samsung smart watch because it is sleek and durable'\n",
    "\n",
    "sentence = Sentence(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run NER over sentence\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"I love Bamboo HR , but interviews make me nervous\"   [− Tokens: 10  − Token-Labels: \"I <S-NP/PRP> love <S-VP/VBP> Bamboo <B-NP/NNP> HR <E-NP/NNP> , <,> but <CC> interviews <S-NP/NNS> make <S-VP/VBP> me <S-NP/PRP> nervous <S-ADJP/JJ>\"]\n"
     ]
    }
   ],
   "source": [
    "# check prediction\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conjunctions(sentence):\n",
    "    '''\n",
    "    Takes Sentence object as input\n",
    "    Returns pos (list), has_conjunction (boolean), break (integer list)\n",
    "    '''\n",
    "    pos = [span.tag for span in sentence.get_spans('pos-fast')]\n",
    "    has_conjunction = 'CC' in pos\n",
    "    breaks = []\n",
    "    if has_conjunction:\n",
    "        for i, val in enumerate(pos):\n",
    "            if val == 'CC':\n",
    "                breaks.append(i)\n",
    "    return pos, has_conjunction, breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, has_conjunction, breaks = get_conjunctions(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRP', 'VBP', 'NNP', 'NNP', ',', 'CC', 'NNS', 'VBP', 'PRP', 'JJ']\n",
      "True\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "print(pos)\n",
    "print(has_conjunction)\n",
    "print(breaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break sentence into pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_spans(spans):\n",
    "    '''\n",
    "    Takes several spans as input\n",
    "    Text only string as output\n",
    "    '''\n",
    "    text = [text.to_original_text() for text in spans]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic to break up sentence into pieces based on heuristic\n",
    "# currently achieved by breaking on conjunctions\n",
    "\n",
    "parts = []\n",
    "spans = sentence.get_spans('pos-fast')\n",
    "current_break = 0\n",
    "\n",
    "for next_cc in breaks:\n",
    "    before_cc = spans[current_break:next_cc]\n",
    "    cc = spans[next_cc]\n",
    "    parts.append({'type': 'phrase', 'text': combine_spans(before_cc)})\n",
    "    parts.append({'type': 'conjunction', 'text': cc.text})\n",
    "    current_break = next_cc\n",
    "\n",
    "last_part = spans[breaks[-1]+1:]\n",
    "parts.append({'type': 'phrase', 'text': combine_spans(last_part)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'phrase', 'text': 'I love Bamboo HR ,'},\n",
       " {'type': 'conjunction', 'text': 'but'},\n",
       " {'type': 'phrase', 'text': 'interviews make me nervous'}]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate sentence pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic to add sentiment scores or semantic information to each piece of sentence\n",
    "\n",
    "for part in parts:\n",
    "    if part['type'] == 'phrase':\n",
    "        sentence = Sentence(part['text'])\n",
    "        classifier.predict(sentence)\n",
    "        part['sentiment'] = sentence.to_dict()['labels'][0]['value']\n",
    "        part['labels'] = sentence.to_dict()['labels'][0]['confidence']\n",
    "    if part['type'] == 'conjunction':\n",
    "        if part['text'] in ('but'):\n",
    "            part['reverse'] = True\n",
    "        elif part['text'] in ('and', 'or'):\n",
    "            part['reverse'] = False\n",
    "        else:\n",
    "            part['reverse'] = None\n",
    "            print('Error.. unknown conjunction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'phrase',\n",
       "  'text': 'I love Bamboo HR ,',\n",
       "  'sentiment': 'POSITIVE',\n",
       "  'labels': 0.9989123344421387},\n",
       " {'type': 'conjunction', 'text': 'but', 'reverse': True},\n",
       " {'type': 'phrase',\n",
       "  'text': 'interviews make me nervous',\n",
       "  'sentiment': 'NEGATIVE',\n",
       "  'labels': 0.913139283657074}]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show color coded sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_text(parts):\n",
    "    '''\n",
    "    Takes annotated sentence pieces as input\n",
    "    Outputs string with color coding and sentiment scores\n",
    "    '''\n",
    "    output = ''\n",
    "    for part in parts:\n",
    "        if ('sentiment' in part.keys()) and (part['sentiment'] == 'POSITIVE'):\n",
    "            output += Fore.GREEN + part['text'] + ' [' +  str(round(part['labels'], 3)) + ']'\n",
    "        elif 'sentiment' in part.keys() and (part['sentiment'] == 'NEGATIVE'):\n",
    "            output += Fore.RED + part['text'] + ' [' +  str(round(part['labels'], 3)) + ']'\n",
    "        else:\n",
    "            output += Fore.BLACK + part['text']\n",
    "        output += ' '\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI love Bamboo HR , [0.999] \u001b[30mbut \u001b[31minterviews make me nervous [0.913] \n"
     ]
    }
   ],
   "source": [
    "print(color_text(parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
